{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaomingtao/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm #Still for graphics\n",
    "import matplotlib.colors # For nice colours\n",
    "import wordcloud\n",
    "import numpy as np\n",
    "import scipy\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import sklearn.manifold\n",
    "import itertools\n",
    "import json\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import spacy #comp-linguistics\n",
    "import gensim#For topic modeling\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "#To process text data\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import statsmodels.formula.api as smf\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "from langdetect import detect\n",
    "from textblob import Word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scrape Youtube\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "from oauth2client.tools import argparser\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to scrape posts links from a give hashtag\n",
    "def scrape_hashtag(hashtag, num_posts):\n",
    "    browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    browser.get('https://www.instagram.com/accounts/login/')\n",
    "    time.sleep(30) #Time to log in for scraping\n",
    "    browser.get('https://www.instagram.com/explore/tags/' + hashtag)\n",
    "    Pagelength = browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    links=[]\n",
    "    \n",
    "    # Repetitively scolling down and scraping new data until it reaches to num_posts\n",
    "    while len(links) <= num_posts:\n",
    "        source = browser.page_source\n",
    "        data = bs(source, 'html.parser')\n",
    "        body = data.find('body')\n",
    "        script = body.find('article')\n",
    "        \n",
    "        # Scrape the link to the post under the hashtag\n",
    "        for link in script.findAll('a'):\n",
    "            if re.match(\"/p\", link.get('href')):\n",
    "                if 'https://www.instagram.com'+link.get('href') not in links:\n",
    "                    links.append('https://www.instagram.com'+link.get('href'))\n",
    "                    \n",
    "        scroll_down = \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "        browser.execute_script(scroll_down)\n",
    "        time.sleep(3)  \n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to generate dataframe from a list of post links\n",
    "def scrape_post(post_lst, ins_data):\n",
    "    browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    browser.get('https://www.instagram.com/accounts/login/')\n",
    "    time.sleep(30) #Time to log in for scraping\n",
    "    \n",
    "    for i in range(len(post_lst)):\n",
    "        info = {}\n",
    "        browser.get(post_lst[i])\n",
    "        time.sleep(25)\n",
    "        source = browser.page_source\n",
    "        soup = bs(source, \"html.parser\")\n",
    "        scripts = soup.find_all(\"script\")\n",
    "                \n",
    "        # If the post link does not contain the post anymore, skip the link\n",
    "        if len(scripts) < 7:\n",
    "            print('no')\n",
    "            return pd.DataFrame(ins_data).T\n",
    "            continue\n",
    "\n",
    "        for script in scripts[21:24]:\n",
    "            if script.contents[0][:29] == \"window.__additionalDataLoaded\":\n",
    "                print(\"found\")\n",
    "\n",
    "            try:\n",
    "                # Find out the information that we need from the script\n",
    "                data = json.loads(script.contents[0][48:-2])\n",
    "                if not data[\"graphql\"][\"shortcode_media\"]:\n",
    "                    print('nooo')\n",
    "                    continue\n",
    "                image_data = data[\"graphql\"][\"shortcode_media\"]\n",
    "                info[\"timestamp\"] = datetime.fromtimestamp(image_data[\"taken_at_timestamp\"])\n",
    "                info[\"caption\"] = str(image_data[\"edge_media_to_caption\"][\"edges\"][0][\"node\"][\"text\"])\n",
    "                info[\"user\"] = str(image_data[\"owner\"][\"username\"])\n",
    "                info[\"full_name\"] = str(image_data[\"owner\"][\"full_name\"])\n",
    "                info[\"likes\"] = image_data[\"edge_media_preview_like\"][\"count\"]\n",
    "                info[\"image_url\"] = str(image_data[\"display_url\"])\n",
    "                info[\"dimensions\"] = image_data[\"dimensions\"]\n",
    "\n",
    "                # Store the post location, but some posts do not have a location in script\n",
    "                location = image_data[\"location\"]\n",
    "                if location:\n",
    "                    if location[\"address_json\"]:\n",
    "                        info[\"location\"] = json.loads(location[\"address_json\"])\n",
    "                        info[\"country\"] = info[\"location\"][\"country_code\"]\n",
    "\n",
    "                # Store tagged account that appeared in the post, some posts do not have\n",
    "                tagged_account = []\n",
    "                for edge in image_data[\"edge_media_to_tagged_user\"][\"edges\"]:\n",
    "                    tagged_account.append(edge[\"node\"][\"user\"][\"username\"])\n",
    "                info[\"tagged_account\"] = tagged_account\n",
    "                ins_data[i] = info\n",
    "            except:\n",
    "                continue\n",
    "    return pd.DataFrame(ins_data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"lululemon\"\n",
    "raw_links = scrape_hashtag(hashtag, 3000)\n",
    "ins_data = scrape_post(raw_links[:50], {})\n",
    "ins_data.to_csv('{}_Instagram.csv'.format(hashtag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = \"AIzaSyDbQYhRdAnrlU0BQ7gpFpsZUiEWtP0WFts\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "youtube = discovery.build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/SeyiAgboola/YouTube-Mining/blob/master/youtube_search.py\n",
    "# Scape videos based on date, rating, relevance, title, viewCount\n",
    "def youtubeSearch(query, max_results=50, order=\"viewCount\", token=None, \n",
    "                  location=None, location_radius=None):\n",
    "    print(token)\n",
    "    search_response = youtube.search().list(\n",
    "        q=query,\n",
    "        type=\"video\",\n",
    "        pageToken=token,\n",
    "        order = order,\n",
    "        videoCaption='closedCaption',\n",
    "        part=\"id,snippet\",\n",
    "        maxResults=max_results,\n",
    "        location=location,\n",
    "        locationRadius=location_radius).execute()\n",
    "\n",
    "    print(\"Search Completed...\")\n",
    "    print(\"Total results: {0} \\nResults per page: {1}\".format(search_response['pageInfo']['totalResults'], \n",
    "                                                              search_response['pageInfo']['resultsPerPage']))\n",
    "    return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeResults(response):\n",
    "    \n",
    "    #create variables to store your values\n",
    "    title = []\n",
    "    channelId = []\n",
    "    channelTitle = []\n",
    "    categoryId = []\n",
    "    videoId = []\n",
    "    viewCount = []\n",
    "    likeCount = []\n",
    "    dislikeCount = []\n",
    "    commentCount = []\n",
    "    favoriteCount = []\n",
    "    category = []\n",
    "    tags = []\n",
    "    videos = []\n",
    "    captions = []\n",
    "    \n",
    "    for search_result in response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "\n",
    "            #append title and video for each item\n",
    "            title.append(search_result['snippet']['title'])\n",
    "            videoId.append(search_result['id']['videoId'])\n",
    "\n",
    "            #then collect stats on each video using videoId\n",
    "            stats = youtube.videos().list(\n",
    "                part='statistics, snippet',\n",
    "                id=search_result['id']['videoId']).execute()\n",
    "            \n",
    "            channelId.append(stats['items'][0]['snippet']['channelId']) \n",
    "            channelTitle.append(stats['items'][0]['snippet']['channelTitle']) \n",
    "            categoryId.append(stats['items'][0]['snippet']['categoryId']) \n",
    "            favoriteCount.append(stats['items'][0]['statistics']['favoriteCount'])\n",
    "            \n",
    "            # Collect viewCounts\n",
    "            try:\n",
    "                viewCount.append(stats['items'][0]['statistics']['viewCount'])\n",
    "            except:\n",
    "                viewCount.append(\"Not available\")\n",
    "            \n",
    "            # Collect likes\n",
    "            try:\n",
    "                likeCount.append(stats['items'][0]['statistics']['likeCount'])\n",
    "            except:\n",
    "                likeCount.append(\"Not available\")\n",
    "                \n",
    "            # Collect dislikes\n",
    "            try:\n",
    "                dislikeCount.append(stats['items'][0]['statistics']['dislikeCount'])     \n",
    "            except:\n",
    "                dislikeCount.append(\"Not available\")\n",
    "                \n",
    "            # Extract subtitles\n",
    "            try:\n",
    "                transcript_list =YouTubeTranscriptApi.list_transcripts(search_result['id']['videoId'])\n",
    "                caption = \"\"\n",
    "                try:\n",
    "                    transcript = transcript_list.find_generated_transcript(['en'])\n",
    "                    caption = extract_captions(transcript)\n",
    "                except:\n",
    "                    try:\n",
    "                        transcipt = transcript_list.find_manually_created_transcript(['en'])\n",
    "                        caption = extract_captions(transcript)\n",
    "                    except:\n",
    "                        captions.append(\"\")\n",
    "                captions.append(caption)\n",
    "            except:\n",
    "                captions.append(\"\")\n",
    "\n",
    "            if 'commentCount' in stats['items'][0]['statistics'].keys():\n",
    "                commentCount.append(stats['items'][0]['statistics']['commentCount'])\n",
    "            else:\n",
    "                commentCount.append(0)\n",
    "         \n",
    "            if 'tags' in stats['items'][0]['snippet'].keys():\n",
    "                tags.append(stats['items'][0]['snippet']['tags'])\n",
    "            else:\n",
    "                #I'm not a fan of empty fields\n",
    "                tags.append(\"No Tags\")\n",
    "                \n",
    "    #Break out of for-loop and if statement and store lists of values in dictionary\n",
    "    youtube_dict = {'tags':tags,'channelId': channelId,'channelTitle': channelTitle,\n",
    "                    'categoryId':categoryId,'title':title,'videoId':videoId,\n",
    "                    'viewCount':viewCount,'likeCount':likeCount,'dislikeCount':dislikeCount,\n",
    "                    'commentCount':commentCount,'favoriteCount':favoriteCount, 'captions':captions}\n",
    " \n",
    "    return youtube_dict\n",
    "\n",
    "def extract_captions(transcript):\n",
    "    caption = \"\"\n",
    "    for text in transcript.fetch():\n",
    "        caption += text['text']\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the result into csv file\n",
    "def writeCSV(results, filename):\n",
    "    import csv\n",
    "    keys = sorted(results.keys())\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as output:\n",
    "        writer = csv.writer(output, delimiter=\",\")\n",
    "        writer.writerow(keys)\n",
    "        writer.writerows(zip(*[results[key] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run YouTube Search\n",
    "q ='nike'\n",
    "response = youtubeSearch(q)\n",
    "results = storeResults(response)\n",
    "writeCSV(results, \"{}_YouTube.csv\".format(q))\n",
    "for i in range(10):\n",
    "    token = response['nextPageToken']\n",
    "    response = youtubeSearch(q, token=token)\n",
    "    results = storeResults(response)\n",
    "    writeCSV(results, \"{}_YouTube.csv\".format(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = \"F6gNbHvgpGFCsg\"\n",
    "SECRET_TOKEN = \"aI6rgzVffEoA9_lNLz-wI4ul6uWx8w\"\n",
    "auth = requests.auth.HTTPBasicAuth('CLIENT_ID', 'SECRET_TOKEN')\n",
    "data = {'grant_type': 'password', 'username': 'mingTG01', 'password': 'gMt0612!'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='F6gNbHvgpGFCsg', client_secret='aI6rgzVffEoA9_lNLz-wI4ul6uWx8w',\n",
    "                     user_agent='gmt_project', username='mingTG01', password='gMt0612!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit(subreddit, limit):\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "    new_subreddit = subreddit.new(limit=limit)\n",
    "    top_subreddit = subreddit.top(limit=limit)\n",
    "    hot_subreddit = subreddit.hot(limit=limit)\n",
    "    \n",
    "    data_dict = { \"title\":[], \"upvotes\":[], \"upvote_ratio\":[], \"url\":[], \"num_commns\": [], \n",
    "                   \"created\": [], \"body\":[]}\n",
    "    data_dict = extract_post(data_dict, new_subreddit)\n",
    "    data_dict = extract_post(data_dict, top_subreddit)\n",
    "    data_dict = extract_post(data_dict, hot_subreddit)\n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    data_df['created'] = data_df['created'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "    return data_df\n",
    "\n",
    "def extract_post(data_dict, subreddit):\n",
    "    for submission in subreddit:\n",
    "        data_dict[\"title\"].append(submission.title)\n",
    "        data_dict[\"upvotes\"].append(submission.score)\n",
    "        data_dict[\"upvote_ratio\"].append(submission.upvote_ratio)\n",
    "        data_dict[\"url\"].append(submission.url)\n",
    "        data_dict[\"num_commns\"].append(submission.num_comments)\n",
    "        data_dict[\"created\"].append(submission.created_utc)\n",
    "        data_dict[\"body\"].append(submission.selftext)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"nike\"\n",
    "df = scrape_subreddit(q, 5000)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('{}_Reddit.csv'.format(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "consumer_key = \"OEma2BDMR252S8QItXpJttqGD\"\n",
    "consumer_secret = \"wK85Jhq67X0WY0e7UShVjeOJNOdG5x8Xv2RuqlCBoVMrzCgTpp\"\n",
    "access_token = \"1248088175102300161-7X0wVvv8Jhf33ykwBozin4RnFCAxc5\"\n",
    "access_token_secret = \"yXYiqKTDwGoLxgEyil9yJfqxDZr9cwt1zCqoVJIrbbkvE\"\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape tweets with keyword\n",
    "keyword = \"gymshark\"\n",
    "tweets = tw.Cursor(api.search, q=keyword, lang='en').items(5000)\n",
    "tweet_details = [[tweet.created_at, tweet.text, tweet.user.screen_name, tweet.user.location,\n",
    "                  tweet.retweet_count, tweet.favorite_count, tweet.entities['hashtags'], \n",
    "                  tweet.entities['user_mentions']] for tweet in tweets]\n",
    "columns = ['time', 'text', 'user_name', 'user_location', 'retweet_cnt', 'favorite_cnt', \n",
    "           'hashtags', 'user_mentions']\n",
    "df = pd.DataFrame(tweet_details, columns = columns)\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: [dic['text'] for dic in x])\n",
    "df['user_mentions'] = df['user_mentions'].apply(lambda x: [dic['screen_name'] for dic in x])\n",
    "df['is_retweet'] = df['text'].apply(lambda x: x[:2] == 'RT')\n",
    "df.to_csv('{}_Twitter.csv'.format(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
